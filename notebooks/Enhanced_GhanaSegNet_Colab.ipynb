{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1125a548",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EricBaidoo/GhanaSegNet/blob/main/notebooks/Enhanced_GhanaSegNet_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0576ce",
   "metadata": {},
   "source": [
    "# üéØ Enhanced GhanaSegNet - Food Segmentation\n",
    "\n",
    "**Objective:** Train Enhanced GhanaSegNet to achieve 30% mIoU for food segmentation\n",
    "\n",
    "## üìã **Project Overview**\n",
    "- **Model**: Enhanced GhanaSegNet with FPN + Advanced ASPP + Multi-Head Attention\n",
    "- **Parameters**: ~10.5M\n",
    "- **Backbone**: EfficientNet-B0\n",
    "- **Target**: 30% mIoU (improvement over 24.37% baseline)\n",
    "- **Dataset**: Ghana Food Segmentation Dataset\n",
    "\n",
    "## üìö **Notebook Structure**\n",
    "1. **Setup & Environment** - Dependencies, paths, verification\n",
    "2. **Data Loading** - Dataset preparation and loaders\n",
    "3. **Model Architecture** - Enhanced GhanaSegNet implementation\n",
    "4. **Training Pipeline** - Real training with optimizations\n",
    "5. **Evaluation & Results** - Performance analysis\n",
    "6. **Test-Time Augmentation** - Optional performance boost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79163045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1Ô∏è‚É£ SETUP & ENVIRONMENT\n",
    "# ========================================\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üîó Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"üìç Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if needed)\n",
    "if 'google.colab' in sys.modules and not os.path.exists('/content/GhanaSegNet'):\n",
    "    print(\"üì• Cloning GhanaSegNet repository...\")\n",
    "    !git clone https://github.com/EricBaidoo/GhanaSegNet.git /content/GhanaSegNet\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "    %cd /content/GhanaSegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß ENVIRONMENT VERIFICATION & SETUP\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç SYSTEM VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Set up paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    PROJECT_ROOT = '/content/GhanaSegNet'\n",
    "    DATA_PATH = '/content/drive/MyDrive/data'\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_PATH = 'data'\n",
    "    print(\"üìç Running locally\")\n",
    "\n",
    "# Add project to Python path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# Verify key files\n",
    "key_files = [\n",
    "    'models/ghanasegnet.py',\n",
    "    'utils/losses.py', \n",
    "    'utils/metrics.py',\n",
    "    'data/dataset_loader.py'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file_path in key_files:\n",
    "    full_path = os.path.join(PROJECT_ROOT, file_path)\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file_path} - MISSING!\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "# Check dataset\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"‚úÖ Dataset directory found\")\n",
    "    if os.path.exists(os.path.join(DATA_PATH, 'train')):\n",
    "        print(f\"‚úÖ Train split available\")\n",
    "    if os.path.exists(os.path.join(DATA_PATH, 'val')):\n",
    "        print(f\"‚úÖ Validation split available\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Dataset not found at: {DATA_PATH}\")\n",
    "\n",
    "if not missing_files:\n",
    "    print(f\"\\nüéâ SETUP COMPLETE - Ready to proceed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some files missing - check repository structure\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d977fd6",
   "metadata": {},
   "source": [
    "---\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ DATA LOADING & PREPARATION\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä DATASET LOADING\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset_loader import GhanaFoodDataset\n",
    "\n",
    "print(\"üìä Loading Ghana Food Dataset...\")\n",
    "\n",
    "try:\n",
    "    # Load datasets\n",
    "    train_dataset = GhanaFoodDataset(DATA_PATH, split='train')\n",
    "    val_dataset = GhanaFoodDataset(DATA_PATH, split='val')\n",
    "    \n",
    "    print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "    print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset loading failed: {e}\")\n",
    "    print(\"Please check your dataset path and structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98424619",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ MODEL ARCHITECTURE\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è ENHANCED GHANASEGNET MODEL\n",
    "\n",
    "from models.ghanasegnet import EnhancedGhanaSegNet\n",
    "from utils.losses import CombinedLoss\n",
    "from utils.metrics import calculate_miou\n",
    "\n",
    "print(\"üèóÔ∏è Initializing Enhanced GhanaSegNet...\")\n",
    "\n",
    "# Initialize model\n",
    "model = EnhancedGhanaSegNet(num_classes=6).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ Model initialized\")\n",
    "print(f\"üìä Parameters: {num_params/1e6:.2f}M\")\n",
    "print(f\"üéØ Architecture: EfficientNet-B0 + FPN + Enhanced ASPP + Multi-Head Attention\")\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = CombinedLoss()\n",
    "print(f\"‚úÖ Combined loss function ready (Dice + Focal + Boundary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ TRAINING PIPELINE\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è TRAINING CONFIGURATION\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚öôÔ∏è Setting up training configuration...\")\n",
    "\n",
    "# Training hyperparameters\n",
    "config = {\n",
    "    'epochs': 15,\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'weight_decay': 1.2e-3,\n",
    "    'batch_size': 8,\n",
    "    'patience': 8,\n",
    "    'save_path': 'best_enhanced_ghanasegnet.pth'\n",
    "}\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=config['learning_rate'], \n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "\n",
    "print(f\"‚úÖ Optimizer: AdamW (lr={config['learning_rate']}, wd={config['weight_decay']})\")\n",
    "print(f\"‚úÖ Scheduler: Cosine Annealing with Warm Restarts\")\n",
    "print(f\"‚úÖ Training for {config['epochs']} epochs\")\n",
    "\n",
    "# Training tracking\n",
    "best_val_iou = 0.0\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [], \n",
    "    'val_iou': [],\n",
    "    'learning_rate': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f719e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ ENHANCED GHANASEGNET TRAINING\n",
    "\n",
    "print(\"üöÄ STARTING ENHANCED GHANASEGNET TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéØ Target: Improve upon baseline (24.37% mIoU)\")\n",
    "print(f\"üî• Model: Enhanced Architecture ({num_params/1e6:.1f}M parameters)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training loop\n",
    "print(\"üîÑ Beginning training...\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]} [Train]') as pbar:\n",
    "        for batch_idx, (images, masks) in enumerate(pbar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Handle potential tuple output from model\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{train_loss/train_batches:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]} [Val]') as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions for mIoU calculation\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_predictions.append(preds.cpu().numpy())\n",
    "                all_targets.append(masks.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    val_iou = calculate_miou(all_predictions, all_targets, num_classes=6)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Store training history\n",
    "    training_history['train_loss'].append(train_loss / train_batches)\n",
    "    training_history['val_loss'].append(val_loss / len(val_loader))\n",
    "    training_history['val_iou'].append(val_iou)\n",
    "    training_history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Display epoch results\n",
    "    print(f\"\\nüöÄ EPOCH {epoch+1}/{config['epochs']}\")\n",
    "    print(f\"üìä Train Loss: {train_loss/train_batches:.4f}\")\n",
    "    print(f\"üìä Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    print(f\"üìä Val IoU: {val_iou:.4f} ({val_iou*100:.2f}%)\")\n",
    "    print(f\"‚ö°Ô∏è Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"‚è±Ô∏è  Epoch Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_iou > best_val_iou:\n",
    "        best_val_iou = val_iou\n",
    "        print(f\"üéØ NEW BEST! Saving model...\")\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_iou': best_val_iou,\n",
    "            'training_history': training_history\n",
    "        }, config['save_path'])\n",
    "        \n",
    "        # Check if target achieved\n",
    "        if val_iou >= 0.30:\n",
    "            print(\"üéâ TARGET ACHIEVED! 30% mIoU reached!\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Training completion\n",
    "print(f\"\\nüèÅ TRAINING COMPLETED!\")\n",
    "print(f\"üèÜ Best Validation IoU: {best_val_iou:.4f} ({best_val_iou*100:.2f}%)\")\n",
    "\n",
    "# Performance analysis\n",
    "baseline_miou = 0.2437\n",
    "improvement = (best_val_iou - baseline_miou) * 100\n",
    "relative_improvement = (improvement / (baseline_miou * 100)) * 100\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE ANALYSIS:\")\n",
    "print(f\"   Baseline: {baseline_miou*100:.2f}% mIoU\")\n",
    "print(f\"   Enhanced: {best_val_iou*100:.2f}% mIoU\")\n",
    "print(f\"   Improvement: +{improvement:.2f} percentage points\")\n",
    "print(f\"   Relative gain: +{relative_improvement:.1f}%\")\n",
    "\n",
    "if best_val_iou >= 0.30:\n",
    "    print(f\"üéâ EXCELLENT! 30% mIoU target achieved!\")\n",
    "elif best_val_iou >= 0.28:\n",
    "    print(f\"üî• GREAT! Very close to 30% target!\")\n",
    "elif best_val_iou >= 0.26:\n",
    "    print(f\"‚úÖ SOLID! Good improvement over baseline!\")\n",
    "else:\n",
    "    print(f\"üìä Training completed. Consider TTA boost for higher performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236246f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 5Ô∏è‚É£ EVALUATION & RESULTS\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä TRAINING RESULTS VISUALIZATION\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Visualizing training results...\")\n",
    "\n",
    "# Create training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training & Validation Loss\n",
    "axes[0, 0].plot(training_history['train_loss'], label='Train Loss', color='blue')\n",
    "axes[0, 0].plot(training_history['val_loss'], label='Val Loss', color='red')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation IoU\n",
    "axes[0, 1].plot(training_history['val_iou'], label='Val IoU', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=0.30, color='red', linestyle='--', label='30% Target')\n",
    "axes[0, 1].axhline(y=best_val_iou, color='orange', linestyle='--', label=f'Best: {best_val_iou:.3f}')\n",
    "axes[0, 1].set_title('Validation IoU Progress')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('IoU')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "axes[1, 0].plot(training_history['learning_rate'], label='Learning Rate', color='purple')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Performance Comparison\n",
    "models = ['Baseline', 'Enhanced GhanaSegNet']\n",
    "performance = [baseline_miou * 100, best_val_iou * 100]\n",
    "colors = ['lightblue', 'darkblue']\n",
    "\n",
    "axes[1, 1].bar(models, performance, color=colors)\n",
    "axes[1, 1].axhline(y=30, color='red', linestyle='--', label='30% Target')\n",
    "axes[1, 1].set_title('Model Performance Comparison')\n",
    "axes[1, 1].set_ylabel('mIoU (%)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(performance):\n",
    "    axes[1, 1].text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà TRAINING SUMMARY:\")\n",
    "print(f\"   Total epochs: {len(training_history['val_iou'])}\")\n",
    "print(f\"   Best epoch: {np.argmax(training_history['val_iou']) + 1}\")\n",
    "print(f\"   Final train loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Best val IoU: {best_val_iou:.4f} ({best_val_iou*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 6Ô∏è‚É£ TEST-TIME AUGMENTATION (OPTIONAL)\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ce5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ TEST-TIME AUGMENTATION BOOST\n",
    "# Run this only if you want to further improve performance\n",
    "\n",
    "print(\"üöÄ APPLYING TEST-TIME AUGMENTATION (TTA)\")\n",
    "print(\"=\"*55)\n",
    "print(\"üí° TTA can provide +1-3% mIoU improvement\")\n",
    "print(\"üî¨ Uses multi-scale and flip augmentations\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the best trained model\n",
    "print(\"üì• Loading best trained model...\")\n",
    "try:\n",
    "    checkpoint = torch.load(config['save_path'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    base_performance = checkpoint['best_val_iou']\n",
    "    print(f\"‚úÖ Loaded model with {base_performance:.4f} ({base_performance*100:.2f}%) mIoU\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Using current model state\")\n",
    "    base_performance = best_val_iou\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def tta_predict(model, x):\n",
    "    \"\"\"Apply Test-Time Augmentation\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original prediction\n",
    "        pred = model(x)\n",
    "        if isinstance(pred, tuple):\n",
    "            pred = pred[0]\n",
    "        predictions.append(F.softmax(pred, dim=1))\n",
    "        \n",
    "        # Horizontal flip\n",
    "        x_flip = torch.flip(x, [3])\n",
    "        pred_flip = model(x_flip)\n",
    "        if isinstance(pred_flip, tuple):\n",
    "            pred_flip = pred_flip[0]\n",
    "        pred_flip = torch.flip(F.softmax(pred_flip, dim=1), [3])\n",
    "        predictions.append(pred_flip)\n",
    "        \n",
    "        # Multi-scale predictions  \n",
    "        for scale in [0.9, 1.1]:\n",
    "            h, w = x.shape[2], x.shape[3]\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            \n",
    "            x_scaled = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "            pred_scaled = model(x_scaled)\n",
    "            if isinstance(pred_scaled, tuple):\n",
    "                pred_scaled = pred_scaled[0]\n",
    "            pred_scaled = F.interpolate(pred_scaled, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            predictions.append(F.softmax(pred_scaled, dim=1))\n",
    "    \n",
    "    return torch.stack(predictions).mean(dim=0)\n",
    "\n",
    "# Apply TTA evaluation\n",
    "print(\"üîÑ Applying TTA to validation set...\")\n",
    "tta_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "all_tta_predictions = []\n",
    "all_tta_targets = []\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(tqdm(tta_loader, desc=\"TTA Evaluation\")):\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    # Apply TTA\n",
    "    tta_preds = tta_predict(model, images)\n",
    "    pred_masks = torch.argmax(tta_preds, dim=1)\n",
    "    \n",
    "    all_tta_predictions.append(pred_masks.cpu().numpy())\n",
    "    all_tta_targets.append(masks.cpu().numpy())\n",
    "\n",
    "# Calculate TTA performance\n",
    "all_tta_predictions = np.concatenate(all_tta_predictions, axis=0)\n",
    "all_tta_targets = np.concatenate(all_tta_targets, axis=0)\n",
    "tta_miou = calculate_miou(all_tta_predictions, all_tta_targets, num_classes=6)\n",
    "\n",
    "# Results\n",
    "improvement = (tta_miou - base_performance) * 100\n",
    "\n",
    "print(f\"\\nüéØ TTA RESULTS:\")\n",
    "print(f\"üìä Base Model: {base_performance:.4f} ({base_performance*100:.2f}% mIoU)\")\n",
    "print(f\"üöÄ With TTA: {tta_miou:.4f} ({tta_miou*100:.2f}% mIoU)\")\n",
    "print(f\"üìà Improvement: +{improvement:.2f} percentage points\")\n",
    "\n",
    "if tta_miou >= 0.30:\n",
    "    print(f\"üéâ EXCELLENT! TTA achieved 30% mIoU target!\")\n",
    "elif tta_miou >= 0.29:\n",
    "    print(f\"üî• OUTSTANDING! Very close to 30% target!\")\n",
    "elif improvement > 1.0:\n",
    "    print(f\"‚úÖ SOLID BOOST! TTA provided meaningful improvement!\")\n",
    "else:\n",
    "    print(f\"üìä TTA applied with modest improvement\")\n",
    "\n",
    "print(f\"\\nüî¨ TTA METHODOLOGY:\")\n",
    "print(f\"   ‚Ä¢ Horizontal flip augmentation\")\n",
    "print(f\"   ‚Ä¢ Multi-scale testing (0.9x, 1.0x, 1.1x)\")\n",
    "print(f\"   ‚Ä¢ Ensemble averaging\")\n",
    "print(f\"   ‚Ä¢ Legitimate evaluation enhancement\")\n",
    "\n",
    "# Final comparison\n",
    "print(f\"\\nüèÜ FINAL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Baseline GhanaSegNet: 24.37% mIoU\")\n",
    "print(f\"   Enhanced GhanaSegNet: {base_performance*100:.2f}% mIoU\")\n",
    "print(f\"   Enhanced + TTA: {tta_miou*100:.2f}% mIoU\")\n",
    "print(f\"   Total improvement: +{(tta_miou - 0.2437)*100:.2f} percentage points\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
