{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1125a548",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EricBaidoo/GhanaSegNet/blob/main/notebooks/Enhanced_GhanaSegNet_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0576ce",
   "metadata": {},
   "source": [
    "# üéØ Enhanced GhanaSegNet - Food Segmentation\n",
    "\n",
    "**Objective:** Train Enhanced GhanaSegNet to achieve 30% mIoU for food segmentation\n",
    "\n",
    "## üìã **Project Overview**\n",
    "- **Model**: Enhanced GhanaSegNet with FPN + Advanced ASPP + Multi-Head Attention\n",
    "- **Parameters**: ~10.5M\n",
    "- **Backbone**: EfficientNet-B0\n",
    "- **Target**: 30% mIoU (improvement over 24.37% baseline)\n",
    "- **Dataset**: Ghana Food Segmentation Dataset\n",
    "\n",
    "## üìö **Notebook Structure**\n",
    "1. **Setup & Environment** - Dependencies, paths, verification\n",
    "2. **Data Loading** - Dataset preparation and loaders\n",
    "3. **Model Architecture** - Enhanced GhanaSegNet implementation\n",
    "4. **Training Pipeline** - Real training with optimizations\n",
    "5. **Evaluation & Results** - Performance analysis\n",
    "6. **Test-Time Augmentation** - Optional performance boost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79163045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1Ô∏è‚É£ SETUP & ENVIRONMENT\n",
    "# ========================================\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üîó Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"üìç Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if needed)\n",
    "if 'google.colab' in sys.modules and not os.path.exists('/content/GhanaSegNet'):\n",
    "    print(\"üì• Cloning GhanaSegNet repository...\")\n",
    "    !git clone https://github.com/EricBaidoo/GhanaSegNet.git /content/GhanaSegNet\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "    %cd /content/GhanaSegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß ENVIRONMENT VERIFICATION & SETUP\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç SYSTEM VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Set up paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    PROJECT_ROOT = '/content/GhanaSegNet'\n",
    "    DATA_PATH = '/content/drive/MyDrive/data'\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_PATH = 'data'\n",
    "    print(\"üìç Running locally\")\n",
    "\n",
    "# Add project to Python path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# Verify key files\n",
    "key_files = [\n",
    "    'models/ghanasegnet.py',\n",
    "    'utils/losses.py', \n",
    "    'utils/metrics.py',\n",
    "    'data/dataset_loader.py'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file_path in key_files:\n",
    "    full_path = os.path.join(PROJECT_ROOT, file_path)\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file_path} - MISSING!\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "# Check dataset\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"‚úÖ Dataset directory found\")\n",
    "    if os.path.exists(os.path.join(DATA_PATH, 'train')):\n",
    "        print(f\"‚úÖ Train split available\")\n",
    "    if os.path.exists(os.path.join(DATA_PATH, 'val')):\n",
    "        print(f\"‚úÖ Validation split available\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Dataset not found at: {DATA_PATH}\")\n",
    "\n",
    "if not missing_files:\n",
    "    print(f\"\\nüéâ SETUP COMPLETE - Ready to proceed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some files missing - check repository structure\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d977fd6",
   "metadata": {},
   "source": [
    "---\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ DATA LOADING & PREPARATION\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä DATASET LOADING - SYNCED WITH TRAIN_BASELINES.PY\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset_loader import GhanaFoodDataset\n",
    "\n",
    "print(\"üìä Loading Ghana Food Dataset (synced with train_baselines.py)...\")\n",
    "\n",
    "try:\n",
    "    # EXACT SAME LOADING AS train_baselines.py\n",
    "    train_dataset = GhanaFoodDataset(DATA_PATH, split='train', data_root=DATA_PATH)\n",
    "    val_dataset = GhanaFoodDataset(DATA_PATH, split='val', data_root=DATA_PATH)\n",
    "    \n",
    "    print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "    print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders with SAME parameters as train_baselines.py\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created successfully (synced)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Primary dataset loading failed: {e}\")\n",
    "    print(\"üîÑ Trying fallback method from train_baselines.py...\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback method from train_baselines.py\n",
    "        train_dataset = GhanaFoodDataset('data', split='train')\n",
    "        val_dataset = GhanaFoodDataset('data', split='val')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"‚úÖ Fallback loading successful\")\n",
    "        print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "        print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå All dataset loading methods failed: {e2}\")\n",
    "        print(\"Please check your dataset path and structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98424619",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ MODEL ARCHITECTURE\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è ENHANCED GHANASEGNET MODEL\n",
    "\n",
    "from models.ghanasegnet import EnhancedGhanaSegNet\n",
    "from utils.losses import CombinedLoss\n",
    "from utils.metrics import calculate_miou\n",
    "\n",
    "print(\"üèóÔ∏è Initializing Enhanced GhanaSegNet...\")\n",
    "\n",
    "# Initialize model\n",
    "model = EnhancedGhanaSegNet(num_classes=6).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ Model initialized\")\n",
    "print(f\"üìä Parameters: {num_params/1e6:.2f}M\")\n",
    "print(f\"üéØ Architecture: EfficientNet-B0 + FPN + Enhanced ASPP + Multi-Head Attention\")\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = CombinedLoss()\n",
    "print(f\"‚úÖ Combined loss function ready (Dice + Focal + Boundary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ TRAINING PIPELINE\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è SYNCED TRAINING CONFIGURATION\n",
    "# EXACTLY matches train_baselines.py enhanced_train_model function\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚öôÔ∏è Setting up training configuration (SYNCED with train_baselines.py)...\")\n",
    "\n",
    "# EXACT SAME parameters as enhanced_train_model in train_baselines.py\n",
    "config = {\n",
    "    'epochs': 15,                    # EXACT match with train_baselines.py\n",
    "    'learning_rate': 2.5e-4,        # EXACT match with train_baselines.py\n",
    "    'weight_decay': 1.2e-3,         # EXACT match with train_baselines.py\n",
    "    'batch_size': 8,                # EXACT match with train_baselines.py\n",
    "    'num_classes': 6,\n",
    "    'device': device,\n",
    "    'disable_early_stopping': True,  # EXACT match with train_baselines.py\n",
    "    'use_cosine_schedule': True,     # EXACT match with train_baselines.py\n",
    "    'use_progressive_training': True, # EXACT match with train_baselines.py\n",
    "    'mixed_precision': True,         # EXACT match with train_baselines.py\n",
    "    'benchmark_mode': True,          # EXACT match with train_baselines.py\n",
    "    'custom_seed': 789,              # EXACT match with train_baselines.py\n",
    "    'save_path': 'checkpoints/enhanced_ghanasegnet/best_model.pth'\n",
    "}\n",
    "\n",
    "# EXACT SAME optimizer initialization as train_baselines.py\n",
    "if config['use_cosine_schedule']:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    # Cosine annealing with warm restarts (from train_baselines.py)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "    print(f\"‚úÖ Cosine annealing scheduler with warmup\")\n",
    "else:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# EXACT SAME loss function as train_baselines.py\n",
    "from utils.losses import CombinedLoss\n",
    "criterion = CombinedLoss(alpha=0.6, aux_weight=0.4, adaptive_weights=True).to(device)\n",
    "print(f\"‚úÖ Advanced boundary-aware loss function (synced)\")\n",
    "\n",
    "print(f\"‚úÖ SYNCED CONFIGURATION:\")\n",
    "print(f\"   üìä Epochs: {config['epochs']} (matches train_baselines.py)\")\n",
    "print(f\"   ‚ö° Learning Rate: {config['learning_rate']} (matches train_baselines.py)\")\n",
    "print(f\"   üõ°Ô∏è  Weight Decay: {config['weight_decay']} (matches train_baselines.py)\")\n",
    "print(f\"   üì¶ Batch Size: {config['batch_size']} (matches train_baselines.py)\")\n",
    "print(f\"   üî• Mixed Precision: {config['mixed_precision']}\")\n",
    "print(f\"   üìà Cosine Schedule: {config['use_cosine_schedule']}\")\n",
    "print(f\"   üéØ Target: 30% mIoU | Realistic: 27-28% mIoU\")\n",
    "\n",
    "# Training tracking\n",
    "best_val_iou = 0.0\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [], \n",
    "    'val_iou': [],\n",
    "    'learning_rate': [],\n",
    "    'epoch_time': []\n",
    "}\n",
    "\n",
    "print(f\"\\nüîÑ Ready for training with EXACT same parameters as your working train_baselines.py!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f719e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ SYNCED TRAINING LOOP\n",
    "# EXACTLY matches the enhanced_train_model function in train_baselines.py\n",
    "\n",
    "print(\"üöÄ ENHANCED GHANASEGNET - AMBITIOUS 15-EPOCH TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéØ TARGET: 30% mIoU | REALISTIC: 27-28% mIoU\")\n",
    "print(f\"üîß ALL OPTIMIZATIONS ACTIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from utils.metrics import compute_iou, compute_pixel_accuracy\n",
    "\n",
    "# Set seed for reproducibility (matching train_baselines.py)\n",
    "torch.manual_seed(config['custom_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config['custom_seed'])\n",
    "\n",
    "# Initialize mixed precision training (matching train_baselines.py)\n",
    "scaler = GradScaler() if config['mixed_precision'] and torch.cuda.is_available() else None\n",
    "\n",
    "# Create checkpoint directory (matching train_baselines.py)\n",
    "import os\n",
    "os.makedirs('checkpoints/enhanced_ghanasegnet', exist_ok=True)\n",
    "\n",
    "# Training loop - EXACT IMPLEMENTATION from train_baselines.py\n",
    "print(\"üîÑ Beginning training (synced with train_baselines.py)...\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ============ TRAINING PHASE ============\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_samples = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\")\n",
    "    for images, masks in train_pbar:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass (EXACT match with train_baselines.py)\n",
    "        if scaler:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    main_output, aux_outputs = outputs\n",
    "                    loss = criterion(main_output, masks, aux_outputs)\n",
    "                else:\n",
    "                    loss = criterion(outputs, masks)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Regular training (EXACT match with train_baselines.py)\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                main_output, aux_outputs = outputs\n",
    "                loss = criterion(main_output, masks, aux_outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_samples += images.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # ============ VALIDATION PHASE ============\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\")\n",
    "        for images, masks in val_pbar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # EXACT validation implementation from train_baselines.py\n",
    "            if scaler:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        main_output = outputs[0]\n",
    "                    else:\n",
    "                        main_output = outputs\n",
    "                    loss = criterion(main_output, masks)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    main_output = outputs[0]\n",
    "                else:\n",
    "                    main_output = outputs\n",
    "                loss = criterion(main_output, masks)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute metrics (EXACT match with train_baselines.py)\n",
    "            iou = compute_iou(main_output, masks)\n",
    "            accuracy = compute_pixel_accuracy(main_output, masks)\n",
    "            \n",
    "            total_iou += iou\n",
    "            total_accuracy += accuracy\n",
    "            val_samples += images.size(0)\n",
    "            \n",
    "            val_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'IoU': f'{iou:.4f}',\n",
    "                'Acc': f'{accuracy:.4f}'\n",
    "            })\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_iou = total_iou / len(val_loader)\n",
    "    avg_val_accuracy = total_accuracy / len(val_loader)\n",
    "    \n",
    "    # Learning rate scheduling (EXACT match with train_baselines.py)\n",
    "    if config['use_cosine_schedule']:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        scheduler.step(avg_val_iou)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Check for new best (EXACT match with train_baselines.py)\n",
    "    is_best = avg_val_iou > best_val_iou\n",
    "    if is_best:\n",
    "        best_val_iou = avg_val_iou\n",
    "        # Save best model (EXACT match with train_baselines.py)\n",
    "        os.makedirs('checkpoints/enhanced_ghanasegnet', exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_iou': best_val_iou,\n",
    "            'config': config\n",
    "        }, 'checkpoints/enhanced_ghanasegnet/best_model.pth')\n",
    "    \n",
    "    # Store training history\n",
    "    training_history['train_loss'].append(avg_train_loss)\n",
    "    training_history['val_loss'].append(avg_val_loss)\n",
    "    training_history['val_iou'].append(avg_val_iou)\n",
    "    training_history['learning_rate'].append(current_lr)\n",
    "    training_history['epoch_time'].append(epoch_time)\n",
    "    \n",
    "    # Progress report (EXACT match with train_baselines.py)\n",
    "    current_miou_percent = avg_val_iou * 100\n",
    "    print(f\"\\nüìä EPOCH {epoch+1}/{config['epochs']} RESULTS:\")\n",
    "    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"   Val IoU: {avg_val_iou:.4f} ({current_miou_percent:.2f}%)\")\n",
    "    print(f\"   Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"   Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"   Best IoU: {best_val_iou:.4f} ({best_val_iou*100:.2f}%)\")\n",
    "    print(f\"   Epoch Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    if is_best:\n",
    "        print(f\"   üéØ NEW BEST PERFORMANCE!\")\n",
    "    \n",
    "    # Check milestones (matching train_baselines.py)\n",
    "    milestone_alerts = [25.0, 27.0, 28.0, 29.0, 30.0]\n",
    "    for milestone in milestone_alerts:\n",
    "        if current_miou_percent >= milestone:\n",
    "            print(f\"\\n\udf89 MILESTONE ACHIEVED: {milestone:.1f}% mIoU!\")\n",
    "            if milestone >= 30.0:\n",
    "                print(f\"üèÜ TARGET REACHED! 30% mIoU ACHIEVED AT EPOCH {epoch+1}!\")\n",
    "    \n",
    "    # Progress toward 30% target\n",
    "    progress_to_target = (current_miou_percent - 24.4) / (30.0 - 24.4) * 100\n",
    "    print(f\"   üìà Progress to 30% target: {progress_to_target:.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Final results (EXACT match with train_baselines.py)\n",
    "print(f\"\\nüèÅ ENHANCED GHANASEGNET 15-EPOCH TRAINING COMPLETE!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\udfaf FINAL RESULTS:\")\n",
    "print(f\"   Best mIoU: {best_val_iou:.4f} ({best_val_iou*100:.2f}%)\")\n",
    "print(f\"   Target: 30.00%\")\n",
    "print(f\"   Gap: {30.0 - best_val_iou*100:+.2f} percentage points\")\n",
    "\n",
    "if best_val_iou >= 0.30:\n",
    "    print(f\"\udfc6 TARGET ACHIEVED! 30%+ mIoU reached!\")\n",
    "elif best_val_iou >= 0.28:\n",
    "    print(f\"üéâ EXCELLENT! Within 2% of target!\")\n",
    "elif best_val_iou >= 0.27:\n",
    "    print(f\"‚úÖ GREAT! Solid improvement achieved!\")\n",
    "else:\n",
    "    print(f\"\udcca Results within expected range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236246f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 5Ô∏è‚É£ EVALUATION & RESULTS\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä TRAINING RESULTS VISUALIZATION\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Visualizing training results...\")\n",
    "\n",
    "# Create training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training & Validation Loss\n",
    "axes[0, 0].plot(training_history['train_loss'], label='Train Loss', color='blue')\n",
    "axes[0, 0].plot(training_history['val_loss'], label='Val Loss', color='red')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation IoU\n",
    "axes[0, 1].plot(training_history['val_iou'], label='Val IoU', color='green', linewidth=2)\n",
    "axes[0, 1].axhline(y=0.30, color='red', linestyle='--', label='30% Target')\n",
    "axes[0, 1].axhline(y=best_val_iou, color='orange', linestyle='--', label=f'Best: {best_val_iou:.3f}')\n",
    "axes[0, 1].set_title('Validation IoU Progress')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('IoU')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "axes[1, 0].plot(training_history['learning_rate'], label='Learning Rate', color='purple')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Performance Comparison\n",
    "models = ['Baseline', 'Enhanced GhanaSegNet']\n",
    "performance = [baseline_miou * 100, best_val_iou * 100]\n",
    "colors = ['lightblue', 'darkblue']\n",
    "\n",
    "axes[1, 1].bar(models, performance, color=colors)\n",
    "axes[1, 1].axhline(y=30, color='red', linestyle='--', label='30% Target')\n",
    "axes[1, 1].set_title('Model Performance Comparison')\n",
    "axes[1, 1].set_ylabel('mIoU (%)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(performance):\n",
    "    axes[1, 1].text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà TRAINING SUMMARY:\")\n",
    "print(f\"   Total epochs: {len(training_history['val_iou'])}\")\n",
    "print(f\"   Best epoch: {np.argmax(training_history['val_iou']) + 1}\")\n",
    "print(f\"   Final train loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Best val IoU: {best_val_iou:.4f} ({best_val_iou*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "# ========================================\n",
    "# 6Ô∏è‚É£ TEST-TIME AUGMENTATION (OPTIONAL)\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ce5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ TEST-TIME AUGMENTATION BOOST\n",
    "# Run this only if you want to further improve performance\n",
    "\n",
    "print(\"üöÄ APPLYING TEST-TIME AUGMENTATION (TTA)\")\n",
    "print(\"=\"*55)\n",
    "print(\"üí° TTA can provide +1-3% mIoU improvement\")\n",
    "print(\"üî¨ Uses multi-scale and flip augmentations\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the best trained model\n",
    "print(\"üì• Loading best trained model...\")\n",
    "try:\n",
    "    checkpoint = torch.load(config['save_path'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    base_performance = checkpoint['best_val_iou']\n",
    "    print(f\"‚úÖ Loaded model with {base_performance:.4f} ({base_performance*100:.2f}%) mIoU\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Using current model state\")\n",
    "    base_performance = best_val_iou\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def tta_predict(model, x):\n",
    "    \"\"\"Apply Test-Time Augmentation\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Original prediction\n",
    "        pred = model(x)\n",
    "        if isinstance(pred, tuple):\n",
    "            pred = pred[0]\n",
    "        predictions.append(F.softmax(pred, dim=1))\n",
    "        \n",
    "        # Horizontal flip\n",
    "        x_flip = torch.flip(x, [3])\n",
    "        pred_flip = model(x_flip)\n",
    "        if isinstance(pred_flip, tuple):\n",
    "            pred_flip = pred_flip[0]\n",
    "        pred_flip = torch.flip(F.softmax(pred_flip, dim=1), [3])\n",
    "        predictions.append(pred_flip)\n",
    "        \n",
    "        # Multi-scale predictions  \n",
    "        for scale in [0.9, 1.1]:\n",
    "            h, w = x.shape[2], x.shape[3]\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            \n",
    "            x_scaled = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "            pred_scaled = model(x_scaled)\n",
    "            if isinstance(pred_scaled, tuple):\n",
    "                pred_scaled = pred_scaled[0]\n",
    "            pred_scaled = F.interpolate(pred_scaled, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            predictions.append(F.softmax(pred_scaled, dim=1))\n",
    "    \n",
    "    return torch.stack(predictions).mean(dim=0)\n",
    "\n",
    "# Apply TTA evaluation\n",
    "print(\"üîÑ Applying TTA to validation set...\")\n",
    "tta_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "all_tta_predictions = []\n",
    "all_tta_targets = []\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(tqdm(tta_loader, desc=\"TTA Evaluation\")):\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    # Apply TTA\n",
    "    tta_preds = tta_predict(model, images)\n",
    "    pred_masks = torch.argmax(tta_preds, dim=1)\n",
    "    \n",
    "    all_tta_predictions.append(pred_masks.cpu().numpy())\n",
    "    all_tta_targets.append(masks.cpu().numpy())\n",
    "\n",
    "# Calculate TTA performance\n",
    "all_tta_predictions = np.concatenate(all_tta_predictions, axis=0)\n",
    "all_tta_targets = np.concatenate(all_tta_targets, axis=0)\n",
    "tta_miou = calculate_miou(all_tta_predictions, all_tta_targets, num_classes=6)\n",
    "\n",
    "# Results\n",
    "improvement = (tta_miou - base_performance) * 100\n",
    "\n",
    "print(f\"\\nüéØ TTA RESULTS:\")\n",
    "print(f\"üìä Base Model: {base_performance:.4f} ({base_performance*100:.2f}% mIoU)\")\n",
    "print(f\"üöÄ With TTA: {tta_miou:.4f} ({tta_miou*100:.2f}% mIoU)\")\n",
    "print(f\"üìà Improvement: +{improvement:.2f} percentage points\")\n",
    "\n",
    "if tta_miou >= 0.30:\n",
    "    print(f\"üéâ EXCELLENT! TTA achieved 30% mIoU target!\")\n",
    "elif tta_miou >= 0.29:\n",
    "    print(f\"üî• OUTSTANDING! Very close to 30% target!\")\n",
    "elif improvement > 1.0:\n",
    "    print(f\"‚úÖ SOLID BOOST! TTA provided meaningful improvement!\")\n",
    "else:\n",
    "    print(f\"üìä TTA applied with modest improvement\")\n",
    "\n",
    "print(f\"\\nüî¨ TTA METHODOLOGY:\")\n",
    "print(f\"   ‚Ä¢ Horizontal flip augmentation\")\n",
    "print(f\"   ‚Ä¢ Multi-scale testing (0.9x, 1.0x, 1.1x)\")\n",
    "print(f\"   ‚Ä¢ Ensemble averaging\")\n",
    "print(f\"   ‚Ä¢ Legitimate evaluation enhancement\")\n",
    "\n",
    "# Final comparison\n",
    "print(f\"\\nüèÜ FINAL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Baseline GhanaSegNet: 24.37% mIoU\")\n",
    "print(f\"   Enhanced GhanaSegNet: {base_performance*100:.2f}% mIoU\")\n",
    "print(f\"   Enhanced + TTA: {tta_miou*100:.2f}% mIoU\")\n",
    "print(f\"   Total improvement: +{(tta_miou - 0.2437)*100:.2f} percentage points\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
