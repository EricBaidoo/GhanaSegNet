{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1125a548",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EricBaidoo/GhanaSegNet/blob/main/notebooks/Enhanced_GhanaSegNet_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0576ce",
   "metadata": {},
   "source": [
    "# Enhanced GhanaSegNet - 30% mIoU Target\n",
    "\n",
    "**Objective:** Train the Enhanced GhanaSegNet architecture to achieve 30% mIoU\n",
    "\n",
    "**Model:** Enhanced GhanaSegNet (FPN + Advanced ASPP + Cross-Attention)\n",
    "- **Parameters:** 10.5M\n",
    "- **Architecture:** EfficientNet-B0 + FPN + Enhanced ASPP + Cross-Attention Transformer\n",
    "- **Target Performance:** 30% mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79163045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and check GPU\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - switch to GPU runtime for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/EricBaidoo/GhanaSegNet.git\n",
    "%cd GhanaSegNet\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"Repository contents:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d59240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dataset loader compatibility for Colab\n",
    "# Ensure the dataset loader supports custom data paths\n",
    "import os\n",
    "\n",
    "# Check if dataset loader needs updating\n",
    "dataset_loader_path = 'data/dataset_loader.py'\n",
    "if os.path.exists(dataset_loader_path):\n",
    "    with open(dataset_loader_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if data_root parameter exists\n",
    "    if 'data_root=None' in content:\n",
    "        print(\"‚úÖ Dataset loader supports custom data paths\")\n",
    "    else:\n",
    "        print(\"üîß Updating dataset loader for custom data paths...\")\n",
    "        # Add data_root parameter support\n",
    "        updated_content = content.replace(\n",
    "            \"def __init__(self, split='train', transform=None, num_classes=6, target_size=(256, 256)):\",\n",
    "            \"def __init__(self, split='train', transform=None, num_classes=6, target_size=(256, 256), data_root=None):\"\n",
    "        ).replace(\n",
    "            \"base_dir = os.path.join(os.path.dirname(__file__), '..', 'data', split)\",\n",
    "            \"\"\"if data_root is None:\n",
    "            data_root = os.path.join(os.path.dirname(__file__), '..', 'data')\n",
    "        base_dir = os.path.join(data_root, split)\"\"\"\n",
    "        )\n",
    "        \n",
    "        with open(dataset_loader_path, 'w') as f:\n",
    "            f.write(updated_content)\n",
    "        print(\"‚úÖ Dataset loader updated for custom data paths\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset loader not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d977fd6",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "**Before running the next cell:**\n",
    "\n",
    "1. **Upload your dataset to Google Drive** in this structure:\n",
    "   ```\n",
    "   MyDrive/\n",
    "     dataset/\n",
    "       train/\n",
    "         images/\n",
    "         masks/\n",
    "       val/\n",
    "         images/\n",
    "         masks/\n",
    "   ```\n",
    "\n",
    "2. **Update the path below** if your dataset is in a different location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract data from Google Drive\n",
    "!cp -r \"/content/drive/MyDrive/data\" .\n",
    "\n",
    "# Verify dataset is copied successfully\n",
    "print(\"Checking dataset structure...\")\n",
    "!ls -la data/\n",
    "print(\"Dataset statistics:\")\n",
    "!echo \"Train images:\" && ls data/train/images/ | wc -l\n",
    "!echo \"Train masks:\" && ls data/train/masks/ | wc -l\n",
    "!echo \"Val images:\" && ls data/val/images/ | wc -l 2>/dev/null || echo \"No val images found\"\n",
    "!echo \"Val masks:\" && ls data/val/masks/ | wc -l 2>/dev/null || echo \"No val masks found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üîß Installing dependencies...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install efficientnet-pytorch opencv-python pillow tqdm\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA: {torch.cuda.is_available()}\")\n",
    "print(\"‚úÖ EfficientNet installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98424619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Enhanced GhanaSegNet can be imported\n",
    "import os\n",
    "os.chdir('/content/GhanaSegNet')\n",
    "\n",
    "try:\n",
    "    from models.ghanasegnet import GhanaSegNet\n",
    "    from utils.losses import CombinedLoss\n",
    "    from utils.metrics import compute_iou\n",
    "    \n",
    "    # Test model creation\n",
    "    model = GhanaSegNet(num_classes=6)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(\"‚úÖ Enhanced GhanaSegNet imported successfully\")\n",
    "    print(f\"‚úÖ Model parameters: {total_params:,} (Target: ~10.5M)\")\n",
    "    print(\"‚úÖ Enhanced loss function ready\")\n",
    "    print(\"‚úÖ All systems ready for 30% mIoU training!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup auto-save to Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = '/content/drive/MyDrive/Enhanced_GhanaSegNet_Results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def save_results():\n",
    "    \"\"\"Save training results to Google Drive\"\"\"\n",
    "    if os.path.exists('checkpoints/ghanasegnet'):\n",
    "        shutil.copytree('checkpoints/ghanasegnet', f'{RESULTS_DIR}/checkpoints', dirs_exist_ok=True)\n",
    "        print(f\"‚úÖ Results saved to: {RESULTS_DIR}\")\n",
    "    else:\n",
    "        print(\"‚ùå No results to save\")\n",
    "\n",
    "print(f\"üìÅ Auto-save configured to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final compatibility check and fix\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Force reload modules to ensure updates are picked up\n",
    "if 'data.dataset_loader' in sys.modules:\n",
    "    del sys.modules['data.dataset_loader']\n",
    "\n",
    "# Double-check dataset loader compatibility\n",
    "try:\n",
    "    from data.dataset_loader import GhanaFoodDataset\n",
    "    \n",
    "    # Test if data_root parameter works\n",
    "    test_dataset = GhanaFoodDataset('train', data_root='data')\n",
    "    print(\"‚úÖ Dataset loader with data_root parameter working correctly\")\n",
    "    \n",
    "except TypeError as e:\n",
    "    if 'data_root' in str(e):\n",
    "        print(\"üîß Applying final dataset loader fix...\")\n",
    "        \n",
    "        # Read and update dataset loader\n",
    "        with open('data/dataset_loader.py', 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Ensure proper data_root support\n",
    "        if 'data_root=None' not in content:\n",
    "            content = content.replace(\n",
    "                \"def __init__(self, split='train', transform=None, num_classes=6, target_size=(256, 256)):\",\n",
    "                \"def __init__(self, split='train', transform=None, num_classes=6, target_size=(256, 256), data_root=None):\"\n",
    "            )\n",
    "        \n",
    "        if 'if data_root is None:' not in content:\n",
    "            content = content.replace(\n",
    "                \"base_dir = os.path.join(os.path.dirname(__file__), '..', 'data', split)\",\n",
    "                \"\"\"if data_root is None:\n",
    "            data_root = os.path.join(os.path.dirname(__file__), '..', 'data')\n",
    "        base_dir = os.path.join(data_root, split)\"\"\"\n",
    "            )\n",
    "        \n",
    "        with open('data/dataset_loader.py', 'w') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        # Force reload\n",
    "        if 'data.dataset_loader' in sys.modules:\n",
    "            del sys.modules['data.dataset_loader']\n",
    "        \n",
    "        print(\"‚úÖ Dataset loader fixed and reloaded\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset loader test failed: {e}\")\n",
    "\n",
    "print(\"üéØ Ready for Enhanced GhanaSegNet training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Enhanced GhanaSegNet - Quick Test (15 epochs)\n",
    "from scripts.train_baselines import train_model\n",
    "\n",
    "# Training configuration for 30% mIoU target\n",
    "config = {\n",
    "    'epochs': 15,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_classes': 6,\n",
    "    'custom_seed': 789,  # Enhanced GhanaSegNet seed\n",
    "    'benchmark_mode': True,\n",
    "    'dataset_path': 'data',\n",
    "    'device': 'cuda',\n",
    "    'timestamp': '2025-10-12',\n",
    "    'note': 'Enhanced GhanaSegNet - 30% mIoU Target'\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting Enhanced GhanaSegNet Training...\")\n",
    "print(\"üéØ Target: 30% mIoU\")\n",
    "print(f\"üìã Config: {config}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Force reload training module to pick up dataset fixes\n",
    "import sys\n",
    "if 'scripts.train_baselines' in sys.modules:\n",
    "    del sys.modules['scripts.train_baselines']\n",
    "\n",
    "from scripts.train_baselines import train_model\n",
    "\n",
    "try:\n",
    "    result = train_model('ghanasegnet', config)\n",
    "    \n",
    "    # Display results\n",
    "    best_iou = result['best_iou']\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ TRAINING COMPLETED!\")\n",
    "    print(f\"üìä Best IoU: {best_iou:.4f} ({best_iou*100:.2f}%)\")\n",
    "    \n",
    "    if best_iou >= 0.30:\n",
    "        print(\"üèÜ 30% mIoU TARGET ACHIEVED! üéâ\")\n",
    "    elif best_iou >= 0.25:\n",
    "        print(f\"üìà Strong Progress! {(best_iou*100):.1f}% (Target: 30%)\")\n",
    "    else:\n",
    "        print(f\"üìä Current: {(best_iou*100):.1f}% (Target: 30%) - Consider full training\")\n",
    "    \n",
    "    # Auto-save results\n",
    "    save_results()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"üîß Trying alternative training approach...\")\n",
    "    \n",
    "    # Alternative: Run training script directly\n",
    "    import subprocess\n",
    "    result = subprocess.run([\n",
    "        'python', 'scripts/train_baselines.py', \n",
    "        '--model', 'ghanasegnet', \n",
    "        '--epochs', '15',\n",
    "        '--dataset-path', 'data',\n",
    "        '--device', 'cuda'\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Training completed via subprocess!\")\n",
    "        save_results()\n",
    "    else:\n",
    "        print(f\"‚ùå Subprocess training also failed: {result.stderr}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c016b",
   "metadata": {},
   "source": [
    "## Full Training (80+ epochs)\n",
    "\n",
    "If the quick test shows promising results, run full training by changing `'epochs': 80` in the config above.\n",
    "\n",
    "**Expected timeline:**\n",
    "- 15 epochs: ~10-15 minutes (quick validation)\n",
    "- 80 epochs: ~45-60 minutes (full training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze results\n",
    "import json\n",
    "import os\n",
    "\n",
    "if os.path.exists('checkpoints/ghanasegnet/training_history.json'):\n",
    "    with open('checkpoints/ghanasegnet/training_history.json', 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    print(\"üìà Training History:\")\n",
    "    print(f\"üìä Final IoU: {history[-1]['val_iou']:.4f}\")\n",
    "    print(f\"üìä Final Accuracy: {history[-1]['val_accuracy']:.4f}\")\n",
    "    print(f\"üìä Best Epoch: {max(history, key=lambda x: x['val_iou'])['epoch']}\")\n",
    "    \n",
    "    # Plot training curves if possible\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        val_iou = [h['val_iou'] for h in history]\n",
    "        train_loss = [h['train_loss'] for h in history]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(epochs, val_iou, 'b-', label='Validation IoU')\n",
    "        ax1.axhline(y=0.30, color='r', linestyle='--', label='30% Target')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('IoU')\n",
    "        ax1.set_title('Enhanced GhanaSegNet - IoU Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.plot(epochs, train_loss, 'g-', label='Training Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Training Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"üìä Install matplotlib for training curves: !pip install matplotlib\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No training history found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9b53c",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "**Enhanced GhanaSegNet Architecture:**\n",
    "- **Backbone:** EfficientNet-B0 (pretrained)\n",
    "- **Decoder:** FPN-style multi-scale fusion\n",
    "- **ASPP:** Advanced with 4 dilation rates [2,4,8,16]\n",
    "- **Attention:** Cross-attention transformer (8+4 heads)\n",
    "- **Loss:** Multi-scale supervision + Dice + Focal + Boundary\n",
    "- **Parameters:** ~10.5M\n",
    "- **Target:** 30% mIoU\n",
    "\n",
    "**Key Innovations:**\n",
    "1. Multi-scale feature pyramid network\n",
    "2. Cross-scale attention mechanism\n",
    "3. Enhanced ASPP with depth-wise convolutions\n",
    "4. Multi-scale auxiliary supervision\n",
    "5. Class-balanced loss for food segmentation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
