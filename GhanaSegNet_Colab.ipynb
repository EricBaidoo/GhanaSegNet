{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d998c88f",
      "metadata": {
        "id": "d998c88f"
      },
      "source": [
        "# GhanaSegNet Colab Training Notebook\n",
        "\n",
        "This notebook sets up your environment, installs dependencies, and runs your baseline training script for UNet, DeepLabV3+, and SegFormer-B0 on Colab GPU.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run each cell in order\n",
        "2. Make sure GPU is enabled: Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "3. Your data should be uploaded to Google Drive or included in your GitHub repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52b4adc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52b4adc6",
        "outputId": "c6716446-e39a-4dd0-8962-77ecbe2f66fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (if your data is stored there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU detected - switch to GPU runtime!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ceb2350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ceb2350",
        "outputId": "b3e14047-0dcf-4c25-e8fc-55c31daa2c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GhanaSegNet'...\n",
            "remote: Enumerating objects: 5486, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 5486 (delta 18), reused 42 (delta 12), pack-reused 5431 (from 2)\u001b[K\n",
            "Receiving objects: 100% (5486/5486), 701.51 MiB | 15.32 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/GhanaSegNet\n",
            "total 308\n",
            "drwxr-xr-x 7 root root  4096 Sep  4 16:58 .\n",
            "drwxr-xr-x 1 root root  4096 Sep  4 16:57 ..\n",
            "-rw-r--r-- 1 root root 14261 Sep  4 16:58 3_WEEK_SPRINT_PLAN.md\n",
            "-rw-r--r-- 1 root root  7588 Sep  4 16:58 ARCHITECTURE_JUSTIFICATION.md\n",
            "-rw-r--r-- 1 root root 17298 Sep  4 16:58 Chapters_1-3.md\n",
            "-rw-r--r-- 1 root root  9657 Sep  4 16:58 GhanaSegNet_Colab.ipynb\n",
            "drwxr-xr-x 8 root root  4096 Sep  4 16:58 .git\n",
            "-rw-r--r-- 1 root root   627 Sep  4 16:58 .gitignore\n",
            "-rw-r--r-- 1 root root 57714 Sep  4 16:58 image.png\n",
            "-rw-r--r-- 1 root root  1139 Sep  4 16:58 kk.ipynb\n",
            "-rw-r--r-- 1 root root    30 Sep  4 16:58 LICENSE\n",
            "drwxr-xr-x 2 root root  4096 Sep  4 16:58 models\n",
            "drwxr-xr-x 2 root root  4096 Sep  4 16:58 notebooks\n",
            "-rw-r--r-- 1 root root   224 Sep  4 16:58 Pipfile\n",
            "-rw-r--r-- 1 root root 67468 Sep  4 16:58 Pipfile.lock\n",
            "-rw-r--r-- 1 root root   717 Sep  4 16:58 pyproject.toml\n",
            "-rw-r--r-- 1 root root  2304 Sep  4 16:58 README.md\n",
            "-rw-r--r-- 1 root root   482 Sep  4 16:58 requirements.txt\n",
            "-rw-r--r-- 1 root root  9434 Sep  4 16:58 RESEARCH_PROPOSAL.md\n",
            "-rw-r--r-- 1 root root  9441 Sep  4 16:58 RESEARCH_TEAM_ANALYSIS.md\n",
            "drwxr-xr-x 2 root root  4096 Sep  4 16:58 scripts\n",
            "-rwxr-xr-x 1 root root  1314 Sep  4 16:58 setup_compute.sh\n",
            "-rw-r--r-- 1 root root  2737 Sep  4 16:58 split_all.py\n",
            "-rw-r--r-- 1 root root  2076 Sep  4 16:58 test_evaluation.py\n",
            "-rw-r--r-- 1 root root   694 Sep  4 16:58 test_import.py\n",
            "-rw-r--r-- 1 root root  3940 Sep  4 16:58 test_training_env.py\n",
            "-rw-r--r-- 1 root root 17542 Sep  4 16:58 TRANSFER_LEARNING_STRATEGY.md\n",
            "-rw-r--r-- 1 root root   604 Sep  4 16:58 tt.py\n",
            "drwxr-xr-x 2 root root  4096 Sep  4 16:58 utils\n"
          ]
        }
      ],
      "source": [
        "# Clone your GitHub repo\n",
        "!git clone https://github.com/EricBaidoo/GhanaSegNet.git\n",
        "%cd GhanaSegNet\n",
        "\n",
        "# Check if we have the expected files\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb7bd9c4",
      "metadata": {
        "id": "fb7bd9c4"
      },
      "source": [
        "## 📁 Dataset Connection Instructions\n",
        "\n",
        "**Before running the next cell:**\n",
        "\n",
        "1. **Locate your data folder in Google Drive** - Find where you uploaded your `data` folder\n",
        "2. **Check the path** - Note the exact path (e.g., `MyDrive/data` or `MyDrive/GhanaSegNet/data`)\n",
        "3. **Update the copy command** - Modify the path in the next cell to match your Drive structure\n",
        "4. **Run the cell** - The dataset will be copied to your Colab workspace\n",
        "\n",
        "**Expected folder structure after copying:**\n",
        "```\n",
        "data/\n",
        "  train/\n",
        "    images/\n",
        "    masks/\n",
        "  val/\n",
        "    images/\n",
        "    masks/\n",
        "  test/ (optional)\n",
        "    images/\n",
        "    masks/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "425c0aed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "425c0aed",
        "outputId": "0434b3c6-5808-4984-8f94-26feb07cd067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/data': No such file or directory\n",
            "Checking dataset structure...\n",
            "ls: cannot access 'data/': No such file or directory\n",
            "Dataset statistics:\n",
            "Train images:\n",
            "ls: cannot access 'data/train/images/': No such file or directory\n",
            "0\n",
            "Train masks:\n",
            "ls: cannot access 'data/train/masks/': No such file or directory\n",
            "0\n",
            "Val images:\n",
            "ls: cannot access 'data/val/images/': No such file or directory\n",
            "0\n",
            "Val masks:\n",
            "ls: cannot access 'data/val/masks/': No such file or directory\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Download and extract data from Google Drive\n",
        "# First, upload your data.tar.gz to Google Drive, then update the path below\n",
        "\n",
        "# Option 1: If you uploaded data.tar.gz to Drive\n",
        "# !cp \"/content/drive/MyDrive/data.tar.gz\" .\n",
        "# !tar -xzf data.tar.gz\n",
        "\n",
        "# Option 2: If you uploaded the data folder directly to Drive\n",
        "# Copy your dataset from Google Drive to Colab workspace\n",
        "# IMPORTANT: Update the path below to match where you uploaded your data folder in Google Drive\n",
        "\n",
        "# Option 1: If your data folder is in the root of MyDrive\n",
        "!cp -r \"/content/drive/MyDrive/Data\" .\n",
        "\n",
        "# Option 2: If your data folder is in a subfolder (update path as needed)\n",
        "# !cp -r \"/content/drive/MyDrive/YourFolder/data\" .\n",
        "\n",
        "# Option 3: If you uploaded a compressed file\n",
        "# !cp \"/content/drive/MyDrive/data.tar.gz\" .\n",
        "# !tar -xzf data.tar.gz\n",
        "\n",
        "# Verify dataset is copied successfully\n",
        "print(\"Checking dataset structure...\")\n",
        "!ls -la data/\n",
        "print(\"Dataset statistics:\")\n",
        "!echo \"Train images:\" && ls data/train/images/ | wc -l\n",
        "!echo \"Train masks:\" && ls data/train/masks/ | wc -l\n",
        "!echo \"Val images:\" && ls data/val/images/ | wc -l 2>/dev/null || echo \"No val images found\"\n",
        "!echo \"Val masks:\" && ls data/val/masks/ | wc -l 2>/dev/null || echo \"No val masks found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b4f20d",
      "metadata": {
        "id": "76b4f20d"
      },
      "outputs": [],
      "source": [
        "# Alternative: Upload data directly to Colab (slower for large datasets)\n",
        "# Uncomment the lines below if you prefer direct upload\n",
        "\n",
        "# from google.colab import files\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Upload your data.zip file\n",
        "# uploaded = files.upload()\n",
        "#\n",
        "# # Extract the uploaded file\n",
        "# for filename in uploaded.keys():\n",
        "#     if filename.endswith('.zip'):\n",
        "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#             zip_ref.extractall('.')\n",
        "#     elif filename.endswith('.tar.gz'):\n",
        "#         !tar -xzf {filename}\n",
        "\n",
        "# print(\"Upload complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854cd112",
      "metadata": {
        "id": "854cd112"
      },
      "outputs": [],
      "source": [
        "# Option 3: Download from Kaggle (after uploading your dataset there)\n",
        "# 1. Upload your data to Kaggle as a public dataset\n",
        "# 2. Get your Kaggle API credentials from kaggle.com/account\n",
        "# 3. Uncomment and run the code below:\n",
        "\n",
        "# !pip install kaggle\n",
        "# from google.colab import files\n",
        "#\n",
        "# # Upload your kaggle.json file\n",
        "# uploaded = files.upload()  # Upload kaggle.json\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "#\n",
        "# # Download your dataset (replace with your dataset path)\n",
        "# !kaggle datasets download yourusername/ghanasegnet-data\n",
        "# !unzip ghanasegnet-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0f3e19",
      "metadata": {
        "id": "df0f3e19"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install additional packages that might be needed\n",
        "!pip install transformers albumentations\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Torchvision: {torchvision.__version__}\")\n",
        "print(f\"Transformers: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea95a46b",
      "metadata": {
        "id": "ea95a46b"
      },
      "outputs": [],
      "source": [
        "# Run training - choose one of the following:\n",
        "\n",
        "# Train all models (this will take a long time)\n",
        "# !python scripts/train_baselines.py --model all --epochs 1\n",
        "\n",
        "# Train individual models:\n",
        "# UNet only\n",
        "# !python scripts/train_baselines.py --model unet --epochs 1\n",
        "\n",
        "# DeepLabV3+ only\n",
        "!python scripts/train_baselines.py --model deeplabv3plus --epochs 1\n",
        "\n",
        "# SegFormer only\n",
        "# !python scripts/train_baselines.py --model segformer --epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20faabe7",
      "metadata": {
        "id": "20faabe7"
      },
      "outputs": [],
      "source": [
        "# Save results and checkpoints to Google Drive\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped folder\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f\"/content/drive/MyDrive/GhanaSegNet_results_{timestamp}\"\n",
        "\n",
        "# Copy checkpoints and results\n",
        "if os.path.exists(\"checkpoints\"):\n",
        "    !mkdir -p \"{save_dir}\"\n",
        "    !cp -r checkpoints \"{save_dir}/\"\n",
        "    !cp -r *.json \"{save_dir}/\" 2>/dev/null || echo \"No JSON files to copy\"\n",
        "    print(f\"Results saved to: {save_dir}\")\n",
        "else:\n",
        "    print(\"No checkpoints directory found - training may have failed\")\n",
        "\n",
        "# List what was saved\n",
        "!ls -la \"{save_dir}\" 2>/dev/null || echo \"Save directory not created\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1624e2",
      "metadata": {
        "id": "dd1624e2"
      },
      "outputs": [],
      "source": [
        "# Check training results\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Check if training summary exists\n",
        "if os.path.exists(\"checkpoints/training_summary.json\"):\n",
        "    with open(\"checkpoints/training_summary.json\", \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    print(\"Training Summary:\")\n",
        "    for model, result in results.items():\n",
        "        print(f\"{model.upper()}: IoU={result['best_iou']:.4f} ({result['status']})\")\n",
        "else:\n",
        "    print(\"No training summary found yet\")\n",
        "\n",
        "# List checkpoint directories\n",
        "if os.path.exists(\"checkpoints\"):\n",
        "    print(\"\\nCheckpoint directories:\")\n",
        "    !ls -la checkpoints/\n",
        "else:\n",
        "    print(\"No checkpoints directory found\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}