{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb72b79",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EricBaidoo/GhanaSegNet/blob/main/Colab_Evaluate_and_Package.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dc2b6",
   "metadata": {},
   "source": [
    "# Colab: Evaluate Checkpoints & Package Results\n",
    "\n",
    "This notebook mounts Google Drive, copies checkpoints/results into the Colab runtime, installs dependencies, runs either a quick \"Path A\" analysis (process existing JSONs) or a full \"Path B\" evaluation (run `analysis/evaluate_checkpoints_per_class.py` on checkpoints), generates plots and per-class JSON outputs, and packages the small outputs back to Drive.\n",
    "\n",
    "How to use:\n",
    "1. Open this notebook in Colab (File > Upload notebook or via `colab.research.google.com` and select this GitHub/Drive file).\n",
    "2. Run cells sequentially. Use Path A if you already have `results/*.json` in Drive. Use Path B to evaluate checkpoints in Drive (requires GPU and the dataset or access to dataset files).\n",
    "\n",
    "This notebook will write a zip file to `/content/drive/My Drive/GhanaSegNet_Results/results_per_class_summary_jsons.zip` containing the small JSON and PNG outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a1a83",
   "metadata": {},
   "source": [
    "## 1) Check for an existing Colab notebook\n",
    "\n",
    "This section searches the repository for other notebooks. It can help avoid duplicates and detect if a Colab-ready notebook already exists.\n",
    "\n",
    "Run the Python cell below to list any `.ipynb` files in the repo and preview the first few lines of any candidate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Search for notebooks in repo\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path('/content/GhanaSegNet') if Path('/content/GhanaSegNet').exists() else Path('.')\n",
    "notebooks = [str(p) for p in Path('.').rglob('*.ipynb')]\n",
    "print(f'Found {len(notebooks)} notebook(s) in repo root:')\n",
    "for nb in notebooks:\n",
    "    print('-', nb)\n",
    "\n",
    "# Preview the first candidate (if any)\n",
    "if notebooks:\n",
    "    with open(notebooks[0], 'r', encoding='utf-8') as f:\n",
    "        first_lines = ''.join([next(f) for _ in range(5)])\n",
    "    print('\\nPreview of', notebooks[0], ':')\n",
    "    print(first_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d989274",
   "metadata": {},
   "source": [
    "## 2) Prepare Colab-compatible notebook metadata\n",
    "\n",
    "If you'd like to programmatically patch notebook metadata to be Colab-friendly (kernelspec and runtime), run the helper below. It uses nbformat to open and rewrite metadata.\n",
    "\n",
    "Note: This step is optional; Colab generally handles metadata itself when opening a notebook from Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: helper to update notebook metadata to be Colab-friendly\n",
    "try:\n",
    "    import nbformat\n",
    "    from nbformat import read, write, v4\n",
    "except Exception as e:\n",
    "    print('nbformat not available, you can pip install nbformat if you want to run this cell')\n",
    "\n",
    "\n",
    "def patch_notebook_metadata(nb_path: str):\n",
    "    nb = nbformat.read(nb_path, as_version=4)\n",
    "    nb.metadata.setdefault('kernelspec', {})\n",
    "    nb.metadata['kernelspec'].update({\n",
    "        'name': 'python3',\n",
    "        'display_name': 'Python 3'\n",
    "    })\n",
    "    nb.metadata.setdefault('colab', {})\n",
    "    nb.metadata['colab'].update({'name': Path(nb_path).name})\n",
    "    nbformat.write(nb, nb_path)\n",
    "    print('Patched metadata for', nb_path)\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# patch_notebook_metadata('Colab_Evaluate_and_Package.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa189d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification & optional automatic run:\n",
    "# This cell will:\n",
    "#  - list copied JSONs and Drive-hosted checkpoint files,\n",
    "#  - if JSONs are present, automatically run Path A analysis,\n",
    "#  - if no JSONs are present and AUTO_RUN_PATH_B is True, copy checkpoints per-model and run Path B evaluations,\n",
    "#  - finally package results and copy the zip back to Drive.\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Configuration: toggle auto-running Path B (full evaluation) when no JSONs are found.\n",
    "# Keep False by default to avoid long compute without explicit consent.\n",
    "AUTO_RUN_PATH_B = False\n",
    "\n",
    "RUNTIME_RESULTS = Path('/content/GhanaSegNet/results')\n",
    "# Resolve DRIVE_CHECKPOINTS_DIR robustly: try globals, then DRIVE_PROJECT_DIR, then a DRIVE_ROOT-based default\n",
    "drv_ck = globals().get('DRIVE_CHECKPOINTS_DIR', None)\n",
    "if not drv_ck:\n",
    "    # try DRIVE_PROJECT_DIR if available\n",
    "    drv_ck = globals().get('DRIVE_PROJECT_DIR', None)\n",
    "if drv_ck:\n",
    "    DRIVE_CKPTS = Path(drv_ck)\n",
    "else:\n",
    "    # fallback to common Drive locations\n",
    "    DRIVE_ROOT = globals().get('DRIVE_ROOT', '/content/drive/My Drive')\n",
    "    DRIVE_CKPTS = Path(DRIVE_ROOT) / 'GhanaSegNet'\n",
    "DRIVE_ROOT = globals().get('DRIVE_ROOT', '/content/drive/My Drive')\n",
    "\n",
    "# Gather JSONs copied to runtime\n",
    "jsons = sorted([p.name for p in RUNTIME_RESULTS.glob('*.json')])\n",
    "\n",
    "# Gather checkpoint file names from Drive (do not copy here)\n",
    "ckpt_files = []\n",
    "if DRIVE_CKPTS.exists():\n",
    "    for model_dir in sorted(DRIVE_CKPTS.iterdir()):\n",
    "        if model_dir.is_dir():\n",
    "            for p in model_dir.glob('*'):\n",
    "                # record relative path so output is readable\n",
    "                try:\n",
    "                    ckpt_files.append(str(p.relative_to(DRIVE_CKPTS)))\n",
    "                except Exception:\n",
    "                    ckpt_files.append(str(p))\n",
    "\n",
    "print('Sample JSONs (first 50):')\n",
    "for n in jsons[:50]:\n",
    "    print(' -', n)\n",
    "print(f'Total JSON files: {len(jsons)}')\n",
    "\n",
    "print('\\nSample checkpoint files (first 50) (from Drive):')\n",
    "for n in ckpt_files[:50]:\n",
    "    print(' -', n)\n",
    "print(f'Total checkpoint files: {len(ckpt_files)}')\n",
    "\n",
    "# If JSONs are present, run Path A analysis automatically (quick path)\n",
    "if len(jsons) > 0:\n",
    "    print('\\nFound JSONs — running Path A analysis (quick).')\n",
    "    try:\n",
    "        # Ensure we're in the workspace with the repo code available\n",
    "        os.chdir('/content/GhanaSegNet') if Path('/content/GhanaSegNet').exists() else None\n",
    "        print('Running analysis/model_comparison_analysis.py')\n",
    "        os.system('python analysis/model_comparison_analysis.py')\n",
    "        print('Running analysis/compute_val_iou_stats.py')\n",
    "        os.system('python analysis/compute_val_iou_stats.py')\n",
    "        print('Path A analysis complete. Results are in results/ directory.')\n",
    "    except Exception as e:\n",
    "        print('Error while running Path A analysis:', e)\n",
    "\n",
    "    # Package results to Drive\n",
    "    OUT_ZIP = Path('/content/results_per_class_summary_jsons.zip')\n",
    "    if OUT_ZIP.exists():\n",
    "        OUT_ZIP.unlink()\n",
    "    shutil.make_archive(str(OUT_ZIP).replace('.zip',''), 'zip', 'results')\n",
    "    DRIVE_OUT = os.path.join(DRIVE_ROOT, 'GhanaSegNet_Results')\n",
    "    Path(DRIVE_OUT).mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(str(OUT_ZIP), os.path.join(DRIVE_OUT, 'results_per_class_summary_jsons.zip'))\n",
    "    print('Packaged results.zip to', os.path.join(DRIVE_OUT, 'results_per_class_summary_jsons.zip'))\n",
    "\n",
    "else:\n",
    "    print('\\nNo JSONs found in runtime results.')\n",
    "    if AUTO_RUN_PATH_B:\n",
    "        print('AUTO_RUN_PATH_B is True — running full Path B across all model folders found in Drive (one-by-one).')\n",
    "        # For each model folder in Drive, copy its checkpoints into runtime and run evaluation, then copy per-model outputs back to Drive.\n",
    "        drive_project = Path(DRIVE_PROJECT_DIR)\n",
    "        models = [p for p in drive_project.iterdir() if p.is_dir()]\n",
    "        if not models:\n",
    "            print('No model folders found under DRIVE_PROJECT_DIR:', DRIVE_PROJECT_DIR)\n",
    "        for model_dir in models:\n",
    "            model_name = model_dir.name\n",
    "            print(f'\\nProcessing model: {model_name}')\n",
    "            dest_ckpt = Path('/content/GhanaSegNet/checkpoints') / model_name\n",
    "            if dest_ckpt.exists():\n",
    "                print('Runtime checkpoint dest already exists; removing and re-copying')\n",
    "                shutil.rmtree(dest_ckpt)\n",
    "            dest_ckpt.mkdir(parents=True, exist_ok=True)\n",
    "            count = 0\n",
    "            for ext in ('*.pth','*.pt','*.ckpt'):\n",
    "                for ck in model_dir.glob(ext):\n",
    "                    shutil.copy(ck, dest_ckpt / ck.name)\n",
    "                    count += 1\n",
    "            print(f'Copied {count} checkpoints for {model_name} to', dest_ckpt)\n",
    "            # Run evaluation for this model (assumes evaluation script writes to results/)\n",
    "            cmd = f'python analysis/evaluate_checkpoints_per_class.py --checkpoints checkpoints/{model_name} --out results/{model_name}_per_class.json'\n",
    "            print('Running:', cmd)\n",
    "            os.system(cmd)\n",
    "            # Optionally remove the copied checkpoints to free space\n",
    "            shutil.rmtree(dest_ckpt)\n",
    "        # After loop, package results\n",
    "        OUT_ZIP = Path('/content/results_per_class_summary_jsons.zip')\n",
    "        if OUT_ZIP.exists():\n",
    "            OUT_ZIP.unlink()\n",
    "        shutil.make_archive(str(OUT_ZIP).replace('.zip',''), 'zip', 'results')\n",
    "        DRIVE_OUT = os.path.join(DRIVE_ROOT, 'GhanaSegNet_Results')\n",
    "        Path(DRIVE_OUT).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(str(OUT_ZIP), os.path.join(DRIVE_OUT, 'results_per_class_summary_jsons.zip'))\n",
    "        print('Packaged and copied results to Drive at', os.path.join(DRIVE_OUT, 'results_per_class_summary_jsons.zip'))\n",
    "    else:\n",
    "        print('AUTO_RUN_PATH_B is False. If you want to run full evaluations on checkpoints, either set AUTO_RUN_PATH_B = True and re-run this cell, or use the helper cell to copy a single model checkpoint and run Path B manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: locate Drive project and count JSONs/checkpoints\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print('--- Drive variable snapshot (from globals) ---')\n",
    "for name in ('DRIVE_ROOT','DRIVE_PROJECT_DIR','DRIVE_RESULTS_DIR','DRIVE_CHECKPOINTS_DIR'):\n",
    "    print(name, '=', globals().get(name, None))\n",
    "\n",
    "# Helper to safely inspect a path\n",
    "def inspect_path(p):\n",
    "    p = Path(p)\n",
    "    out = {'path': str(p), 'exists': p.exists(), 'is_dir': p.is_dir(), 'json_count': 0, 'ckpt_count': 0, 'sample': []}\n",
    "    if p.exists() and p.is_dir():\n",
    "        try:\n",
    "            out['json_count'] = sum(1 for _ in p.rglob('*.json'))\n",
    "            out['ckpt_count'] = sum(1 for _ in p.rglob('*.pth')) + sum(1 for _ in p.rglob('*.pt')) + sum(1 for _ in p.rglob('*.ckpt'))\n",
    "            out['sample'] = [str(x.relative_to(p)) for i,x in enumerate(p.rglob('*')) if i < 20]\n",
    "        except Exception as e:\n",
    "            out['error'] = str(e)\n",
    "    return out\n",
    "\n",
    "# Inspect the main configured paths (if set)\n",
    "candidates = []\n",
    "for var in ('DRIVE_RESULTS_DIR','DRIVE_CHECKPOINTS_DIR','DRIVE_PROJECT_DIR','DRIVE_ROOT'):\n",
    "    v = globals().get(var, None)\n",
    "    if v:\n",
    "        candidates.append((var, v))\n",
    "\n",
    "for var, path in candidates:\n",
    "    info = inspect_path(path)\n",
    "    print('\\n==', var, '=>', info['path'], '==')\n",
    "    print(' exists:', info['exists'], ' is_dir:', info['is_dir'])\n",
    "    if info.get('error'):\n",
    "        print(' error inspecting path:', info['error'])\n",
    "    else:\n",
    "        print(' json_count:', info['json_count'], ' ckpt_count:', info['ckpt_count'])\n",
    "        print(' sample entries (up to 20):')\n",
    "        for s in info['sample'][:20]:\n",
    "            print('  -', s)\n",
    "\n",
    "# If nothing useful, run a targeted search under DRIVE_ROOT for likely folders\n",
    "ROOT = Path(globals().get('DRIVE_ROOT', '/content/drive'))\n",
    "print('\\nSearching under DRIVE_ROOT for likely project/result folders (this may take a few seconds)')\n",
    "found = []\n",
    "if ROOT.exists():\n",
    "    for p in ROOT.rglob('*'):\n",
    "        if p.is_dir() and any(k in p.name.lower() for k in ('ghanas','ghana','ghanasegnet','ghanasgnet','ghanasgnet_results','ghana_seg','ghana_segnet','ghanasgnet_results','results')):\n",
    "            found.append(p)\n",
    "    found = sorted(set(found))\n",
    "    print('Found', len(found), 'candidate directories:')\n",
    "    for p in found[:50]:\n",
    "        js = sum(1 for _ in p.rglob('*.json')) if p.exists() else 0\n",
    "        ck = (sum(1 for _ in p.rglob('*.pth')) + sum(1 for _ in p.rglob('*.pt')) + sum(1 for _ in p.rglob('*.ckpt'))) if p.exists() else 0\n",
    "        print(' -', str(p), f'(jsons={js}, ckpts={ck})')\n",
    "else:\n",
    "    print('DRIVE_ROOT does not exist:', ROOT)\n",
    "\n",
    "print('\\nDiagnostic complete. If you see a candidate path that holds your JSONs or checkpoints, set DRIVE_PROJECT_DIR to that folder (or update DRIVE_RESULTS_DIR/DRIVE_CHECKPOINTS_DIR) and re-run the normalization and verification cells.')\n",
    "\n",
    "# End diagnostic cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: list model folders and checkpoint/json counts under the detected Drive project\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Resolve a candidate project path (try DRIVE_PROJECT_DIR, then DRIVE_CHECKPOINTS_DIR, then DRIVE_ROOT)\n",
    "candidate = globals().get('DRIVE_PROJECT_DIR') or globals().get('DRIVE_CHECKPOINTS_DIR') or globals().get('DRIVE_ROOT') or '/content/drive/My Drive'\n",
    "proj = Path(candidate)\n",
    "print('Listing model folders under:', proj)\n",
    "\n",
    "if not proj.exists():\n",
    "    print('Path does not exist. Did you mount Drive and set DRIVE_PROJECT_DIR?')\n",
    "else:\n",
    "    subdirs = sorted([p for p in proj.iterdir() if p.is_dir()])\n",
    "    if not subdirs:\n",
    "        print('No subfolders found under', proj)\n",
    "    else:\n",
    "        print(f'Found {len(subdirs)} subfolders under {proj}:')\n",
    "        print()\n",
    "        for p in subdirs:\n",
    "            try:\n",
    "                json_count = sum(1 for _ in p.rglob('*.json'))\n",
    "                ckpt_count = sum(1 for _ in p.rglob('*.pth')) + sum(1 for _ in p.rglob('*.pt')) + sum(1 for _ in p.rglob('*.ckpt'))\n",
    "            except Exception as _e:\n",
    "                json_count = 0\n",
    "                ckpt_count = 0\n",
    "            print(f'- {p.name:30} jsons={json_count:4}  ckpts={ckpt_count:4}  path={str(p)}')\n",
    "\n",
    "        print('\n",
    "If you see a model you want to evaluate with Path B, set DRIVE_PROJECT_DIR to the parent folder above (or set DRIVE_CHECKPOINTS_DIR), then use the helper that copies a single checkpoint into the runtime.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63f573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711421f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68203802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a7e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed42ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2ced7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7f101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
